{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6122531,"sourceType":"datasetVersion","datasetId":3509534}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim\nfrom torchvision import datasets, transforms, models\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom efficientnet_pytorch import EfficientNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define Directories","metadata":{}},{"cell_type":"code","source":"train_folder = \"/kaggle/input/fake-or-real-dataset/train/\"\nval_folder = \"/kaggle/input/fake-or-real-dataset/test/\"\nepochs = 100\nlr = 0.003","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare Datasets","metadata":{}},{"cell_type":"code","source":"# Define transforms\ntransform_enb3 = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load dataset\nfull_data = datasets.ImageFolder(root=train_folder, transform=transform_enb3)\nfull_data.classes[0] = 1 # set fake as 1\nfull_data.classes[1] = 0 # set real as 0\n\nval_data = datasets.ImageFolder(root=val_folder, transform=transform_enb3)\nval_data.classes[0] = 1 # set fake as 1\n\n# Define train-test split ratio\ntrain_size = int(0.8 * len(full_data))\ntest_size = len(full_data) - train_size\n\n# Split the dataset\ntrain_data, test_data = torch.utils.data.random_split(full_data, [train_size, test_size])\n\n# Create DataLoader for each dataset\nbatch_size = 256\ntrain_loader = DataLoader(train_data, batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size, shuffle=False)\nval_loader = DataLoader(val_data, batch_size, shuffle=False)\n\n# Print number of samples in each split\nprint(f'Training samples: {len(train_data)}')\nprint(f'Testing samples: {len(test_data)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"testing dataloaders with 5 random photos","metadata":{}},{"cell_type":"code","source":"i = 0\nfor images, labels in train_loader:  # Assuming train_loader gives a batch of images and labels\n    if i >= 5:\n        break\n    \n    # Access the first image in the batch and permute dimensions\n    image = images[0].permute(1, 2, 0)\n    \n    # Display the image\n    plt.imshow(image)\n    plt.title(\"real\" if labels[0].item() == 1 else \"fake\")\n    plt.show()\n    \n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n# Training loop\ndef train_model(model, criterion, optimizer, dataloaders, classification_layer,finetuning_lr,num_epochs=10):\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            # Use tqdm to wrap the dataloader\n            phase_dataloader = tqdm(dataloaders[phase], desc=phase)\n            for inputs, labels in phase_dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device).float().unsqueeze(1)\n                \n                # Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # Forward\n                if phase == \"train\":\n                    if epoch >= 0.8 * num_epochs:\n                        # Unfreeze all layers for fine-tuning\n                        for param in model.parameters():\n                            param.requires_grad = True\n                        optimizer.param_groups[0]['lr'] = finetuning_lr\n                    else:\n                        # Freeze the CNN part\n                        for param in model.parameters():\n                            param.requires_grad = False\n                        # Unfreeze the classification layer\n                        for param in classification_layer.parameters():\n                            param.requires_grad = True\n                elif phase == \"val\":\n                    for param in model.parameters():\n                            param.requires_grad = False\n\n                        \n                outputs = model(inputs)\n                preds = torch.sigmoid(outputs)\n                preds = (preds > 0.5).float()\n#                 print(preds)\n                loss = criterion(outputs, labels)\n\n                # Backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n        \n        print()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef test_model(model, dataloader, criterion, device):\n    model.eval()  # Set model to evaluation mode\n    running_loss = 0.0\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        val_dataloader = tqdm(dataloader, desc=\"validation\")\n        for inputs, labels in val_dataloader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            labels = torch.ones(outputs.size()).to(device)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n\n            preds = (torch.sigmoid(outputs) > 0.5) * 1\n#             print(preds)\n            all_labels.append(labels)\n            all_preds.append(preds)\n\n    # Move all predictions and labels to CPU once\n    all_labels = torch.cat(all_labels).cpu().numpy()\n    all_preds = torch.cat(all_preds).cpu().numpy()\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n#     print(all_labels)\n#     print(all_preds)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds)\n    recall = recall_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n\n    print(f'Test Loss: {epoch_loss:.4f}')\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n    print(f'F1 Score: {f1:.4f}')\n\n    return epoch_loss, accuracy, precision, recall, f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\ndef save_model(model, model_name,model_evaluation_history):\n    model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n    # Define the string date format.\n    # Get the current Date and Time in a DateTime Object.\n    # Convert the DateTime object to string according to the style mentioned in date_time_format string.\n    date_time_format = '%Y_%m_%d__%H_%M_%S'\n    current_date_time_dt = dt.datetime.now()\n    current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n    # Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n    model_file_name = f'{model_name}___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.pth'\n\n    # Save your Model.\n    torch.save(model.state_dict(), f'{model_file_name}_entire_dict')\n    torch.save(model, f'{model_file_name}_entire')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **EfficientNet Training**","metadata":{}},{"cell_type":"code","source":"\nmodel = efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\ndataloaders = {'train' : train_loader, 'val' : test_loader}\ndevice = \"cuda\"\n# Modify the classifier to fit the binary classification problem\nnum_features = model.classifier.fc.in_features\nmodel.classifier.fc = nn.Linear(num_features, 1)  # Output layer for binary classification\n\n# Move the model to the appropriate device\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel = train_model(model, criterion, optimizer, dataloaders, model.classifier.fc ,10e-6)\n\nepoch_loss, accuracy, precision, recall, f1 = test_model(model, val_loader, criterion, device)\n\nsave_model(model, \"efficientnet-B0\", (epoch_loss, accuracy))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **ResNet Training**","metadata":{}},{"cell_type":"code","source":"model = models.resnet18(pretrained=True)\ndataloaders = {'train' : train_loader, 'val' : test_loader}\ndevice = \"cuda\"\n# Modify the classifier to fit the binary classification problem\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 1)  # Output layer for binary classification\n\n# Move the model to the appropriate device\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel = train_model(model, criterion, optimizer, dataloaders,model.fc ,10e-6)\n\nepoch_loss, accuracy, precision, recall, f1 = test_model(model, val_loader, criterion, device)\n\nsave_model(model, \"ResNet-18\", (epoch_loss, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **ResNext**","metadata":{}},{"cell_type":"code","source":"# Define the dataloaders\ndataloaders = {'train': train_loader, 'val': test_loader}\n\n# Set device to GPU if available\ndevice = torch.device(\"cuda\")\n\n# Load the ResNeXt model with pretrained weights\nmodel = models.resnext50_32x4d(pretrained=True)\n\n# Modify the classifier to fit the binary classification problem\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 1)  # Output layer for binary classification\n\n# Move the model to the appropriate device\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel = train_model(model, criterion, optimizer, dataloaders, model.fc, 10e-6, num_epochs=2)\n\nepoch_loss, accuracy, precision, recall, f1 = test_model(model, val_loader, criterion, device)\n\nsave_model(model, \"resnext50_32x4d\", (epoch_loss, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **VGG**","metadata":{}},{"cell_type":"code","source":"# Define the dataloaders\ndataloaders = {'train': train_loader, 'val': test_loader}\n\n# Set device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the VGG model with pretrained weights\nmodel = models.vgg16(pretrained=True)\n\n# Modify the classifier to fit the binary classification problem\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_features, 1)  # Output layer for binary classification\n\n# Move the model to the appropriate device\nmodel = model.to(device)\n\n# Loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel = train_model(model, criterion, optimizer, dataloaders, model.classifier[6], 10e-6)\n\nepoch_loss, accuracy, precision, recall, f1 = test_model(model, val_loader, criterion, device)\n\nsave_model(model, \"vgg16\", (epoch_loss, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}